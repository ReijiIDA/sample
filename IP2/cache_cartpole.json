{
  "Default": null,
  "Simple Stability": "\ndef compute_reward(obs, terminated, truncated, info):\n    # obs[2] is theta\n    theta = obs[2]\n    reward = 1.0 - abs(theta)\n    if terminated: reward = -10.0\n    return reward\n",
  "LLM_gpt-4o-mini": "def compute_reward(obs, terminated, truncated, info):\n    # Define constants for reward scaling\n    x_error_weight = 10.0  # Weight for x_error\n    theta_weight = 5.0      # Weight for pole angle (theta)\n    \n    # Calculate the absolute errors\n    x_error = abs(info['x_error'])\n    theta_error = abs(obs[2])\n    \n    # Calculate the reward based on errors\n    reward = (1.0 / (1.0 + x_error_weight * x_error)) + (1.0 / (1.0 + theta_weight * theta_error))\n    \n    # Penalize if the episode is terminated or truncated\n    if terminated or truncated:\n        reward = -1.0  # A negative reward for termination\n    \n    return float(reward)",
  "LLM_gemini-2.0-flash-001": "import numpy as np\n\ndef compute_reward(obs, terminated, truncated, info):\n    \"\"\"\n    Computes the reward for the CartPole task with a moving sinusoidal target.\n\n    Args:\n        obs (np.ndarray): Observation array containing cart position, cart velocity,\n                           pole angle, and pole angular velocity.\n        terminated (bool): True if the episode terminated due to failure.\n        truncated (bool): True if the episode truncated due to reaching the time limit.\n        info (dict): Dictionary containing additional information, including 'x_error'.\n\n    Returns:\n        float: The reward value.\n    \"\"\"\n    x_error = info['x_error']\n    theta = obs[2]\n\n    # Reward for tracking the target (x_error close to 0)\n    reward_distance = np.exp(-abs(x_error))\n\n    # Reward for keeping the pole upright (theta close to 0)\n    reward_stability = np.exp(-abs(theta))\n\n    # Combine the rewards\n    reward = 0.5 * reward_distance + 0.5 * reward_stability\n\n    # Penalize termination (failure)\n    if terminated:\n        reward = -1.0\n\n    return float(reward)",
  "LLM_llama-3.3-70b-instruct": "import numpy as np\n\ndef compute_reward(obs, terminated, truncated, info):\n    # Define the reward weights\n    x_error_weight = 1.0\n    theta_weight = 0.5\n    \n    # Calculate the rewards\n    x_error_reward = -np.abs(info['x_error'])\n    theta_reward = -np.abs(obs[2])\n    \n    # Combine the rewards\n    reward = x_error_weight * x_error_reward + theta_weight * theta_reward\n    \n    # If the episode is terminated, penalize the agent\n    if terminated:\n        reward -= 10.0\n    \n    return reward"
}